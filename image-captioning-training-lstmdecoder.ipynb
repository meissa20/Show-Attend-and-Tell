{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12984839,"sourceType":"datasetVersion","datasetId":8171424},{"sourceId":13007869,"sourceType":"datasetVersion","datasetId":8171456}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:27.296873Z","iopub.execute_input":"2025-09-09T12:06:27.297114Z","iopub.status.idle":"2025-09-09T12:06:30.266786Z","shell.execute_reply.started":"2025-09-09T12:06:27.297088Z","shell.execute_reply":"2025-09-09T12:06:30.265962Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tensorboardX import SummaryWriter\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import transforms\nimport argparse, json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom nltk.translate.bleu_score import corpus_bleu\n\nimport sys\nsys.path.append(\"/kaggle/input/train-requirements\")\nfrom dataset import ImageCaptionDataset\nfrom decoder import Decoder\nfrom encoder import Encoder\nfrom utils import AverageMeter, accuracy, calculate_caption_lengths, collate_fn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:30.267733Z","iopub.execute_input":"2025-09-09T12:06:30.267950Z","iopub.status.idle":"2025-09-09T12:06:40.109371Z","shell.execute_reply.started":"2025-09-09T12:06:30.267926Z","shell.execute_reply":"2025-09-09T12:06:40.108785Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:40.110090Z","iopub.execute_input":"2025-09-09T12:06:40.110465Z","iopub.status.idle":"2025-09-09T12:06:40.154169Z","shell.execute_reply.started":"2025-09-09T12:06:40.110447Z","shell.execute_reply":"2025-09-09T12:06:40.153557Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='Show, Attend and Tell')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='batch size for training (default: 64)')\nparser.add_argument('--epochs', type=int, default=10, metavar='E',\n                    help='number of epochs to train for (default: 20)')\nparser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n                    help='learning rate of the decoder (default: 1e-3)')\nparser.add_argument('--step-size', type=int, default=5,\n                    help='step size for learning rate annealing (default: 5)')\nparser.add_argument('--alpha-c', type=float, default=1, metavar='A',\n                    help='regularization constant (default: 1)')\nparser.add_argument('--log-interval', type=int, default=100, metavar='L',\n                    help='number of batches to wait before logging training stats (default: 100)')\nparser.add_argument('--data', type=str, default='/kaggle/input/image-captioning-dataset',\n                    help='path to data images (default: /kaggle/input/flicker30k-dataset)')\nparser.add_argument('--network', choices=['vgg19', 'resnet152', 'densenet161'], default='vgg19',\n                    help='Network to use in the encoder (default: vgg19)')\nparser.add_argument('--model', type=str, help='path to model')\nparser.add_argument('--tf', action='store_true', default=False,\n                    help='Use teacher forcing when training LSTM (default: False)')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:40.156143Z","iopub.execute_input":"2025-09-09T12:06:40.156352Z","iopub.status.idle":"2025-09-09T12:06:40.173284Z","shell.execute_reply.started":"2025-09-09T12:06:40.156334Z","shell.execute_reply":"2025-09-09T12:06:40.172542Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"_StoreTrueAction(option_strings=['--tf'], dest='tf', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Use teacher forcing when training LSTM (default: False)', metavar=None)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"args, unknown = parser.parse_known_args()\n\nprint(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:40.173947Z","iopub.execute_input":"2025-09-09T12:06:40.174186Z","iopub.status.idle":"2025-09-09T12:06:40.187072Z","shell.execute_reply.started":"2025-09-09T12:06:40.174146Z","shell.execute_reply":"2025-09-09T12:06:40.186392Z"}},"outputs":[{"name":"stdout","text":"Namespace(batch_size=64, epochs=10, lr=0.001, step_size=5, alpha_c=1, log_interval=100, data='/kaggle/input/image-captioning-dataset', network='vgg19', model=None, tf=False)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tf = True # teacher forcing\nnetwork = \"resnet152\" # other options [\"densenet161\", \"vgg19\"]\narg_lr = 0.0001\nstep_size = 5\ndata = \"/kaggle/input/image-captioning-dataset/data\"\nbatch_size = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:40.187761Z","iopub.execute_input":"2025-09-09T12:06:40.188027Z","iopub.status.idle":"2025-09-09T12:06:40.201714Z","shell.execute_reply.started":"2025-09-09T12:06:40.188009Z","shell.execute_reply":"2025-09-09T12:06:40.200925Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"writer = SummaryWriter()\n\nword_dict = json.load(open(\"/kaggle/input/image-captioning-dataset/word_dict.json\", 'r'))\nvocabulary_size = len(word_dict)\n\nencoder = Encoder(args.network)\ndecoder = Decoder(vocabulary_size, encoder.dim, args.tf)\n\nencoder.to(device)\ndecoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:40.202497Z","iopub.execute_input":"2025-09-09T12:06:40.203192Z","iopub.status.idle":"2025-09-09T12:06:45.059053Z","shell.execute_reply.started":"2025-09-09T12:06:40.203168Z","shell.execute_reply":"2025-09-09T12:06:45.058424Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 202MB/s] \n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Decoder(\n  (init_h): Linear(in_features=512, out_features=512, bias=True)\n  (init_c): Linear(in_features=512, out_features=512, bias=True)\n  (tanh): Tanh()\n  (f_beta): Linear(in_features=512, out_features=512, bias=True)\n  (sigmoid): Sigmoid()\n  (deep_output): Linear(in_features=512, out_features=5507, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (attention): Attention(\n    (U): Linear(in_features=512, out_features=512, bias=True)\n    (W): Linear(in_features=512, out_features=512, bias=True)\n    (v): Linear(in_features=512, out_features=1, bias=True)\n    (tanh): Tanh()\n    (softmax): Softmax(dim=1)\n  )\n  (embedding): Embedding(5507, 512, padding_idx=0)\n  (lstm): LSTMCell(1024, 512)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"optimizer = optim.Adam(decoder.parameters(), lr=args.lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size)\ncross_entropy_loss = nn.CrossEntropyLoss().to(device)\n# cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=0).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.059735Z","iopub.execute_input":"2025-09-09T12:06:45.059949Z","iopub.status.idle":"2025-09-09T12:06:45.064377Z","shell.execute_reply.started":"2025-09-09T12:06:45.059930Z","shell.execute_reply":"2025-09-09T12:06:45.063758Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.065171Z","iopub.execute_input":"2025-09-09T12:06:45.065436Z","iopub.status.idle":"2025-09-09T12:06:45.084800Z","shell.execute_reply.started":"2025-09-09T12:06:45.065413Z","shell.execute_reply":"2025-09-09T12:06:45.084019Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# print(decoder.state_dict())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.085451Z","iopub.execute_input":"2025-09-09T12:06:45.085757Z","iopub.status.idle":"2025-09-09T12:06:45.099235Z","shell.execute_reply.started":"2025-09-09T12:06:45.085740Z","shell.execute_reply":"2025-09-09T12:06:45.098596Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    ImageCaptionDataset(data_transforms, args.data),\n    batch_size=args.batch_size, shuffle=True, num_workers=1)\n\nval_loader = torch.utils.data.DataLoader(\n    ImageCaptionDataset(data_transforms, args.data, split_type='val'),\n    batch_size=args.batch_size, shuffle=True, num_workers=1, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.099919Z","iopub.execute_input":"2025-09-09T12:06:45.100120Z","iopub.status.idle":"2025-09-09T12:06:45.304703Z","shell.execute_reply.started":"2025-09-09T12:06:45.100105Z","shell.execute_reply":"2025-09-09T12:06:45.304066Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train(epoch, encoder, decoder, optimizer, cross_entropy_loss, data_loader, word_dict, alpha_c, log_interval, writer):\n    encoder.eval()\n    decoder.train()\n\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    for batch_idx, (imgs, captions, captions_len) in enumerate(data_loader):\n        imgs, captions = imgs.to(device), captions.to(device)\n        img_features = encoder(imgs)\n        optimizer.zero_grad()\n        preds, alphas = decoder(img_features, captions)\n        targets = captions[:, 1:]    # as the first word is <start>\n        # print(f\"t_bef{targets}\")\n        # print(f\"p_bef{preds[0]}\")\n        targets = pack_padded_sequence(targets, captions_len, batch_first=True, enforce_sorted=False)[0]\n        preds = pack_padded_sequence(preds, captions_len, batch_first=True, enforce_sorted=False)[0]\n        # print(f\"t_after{targets}\")\n        # print(f\"p_after{preds}\")\n        # break\n        att_regularization = alpha_c * ((1 - alphas.sum(1))**2).mean()\n        \n        loss = cross_entropy_loss(preds, targets)\n        loss += att_regularization\n        loss.backward()    # computer gradients\n        optimizer.step()   # update weights\n\n        total_caption_length = calculate_caption_lengths(word_dict, captions)\n        acc1 = accuracy(preds, targets, 1)  # was the most probable word = target?\n        acc5 = accuracy(preds, targets, 5)  # was the target in the top 5 most probable words?\n        losses.update(loss.item(), total_caption_length)\n        top1.update(acc1, total_caption_length)\n        top5.update(acc5, total_caption_length)\n\n        if batch_idx % log_interval == 0:\n            print('Train Batch: [{0}/{1}]\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top 1 Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'\n                  'Top 5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n                      batch_idx, len(data_loader), loss=losses, top1=top1, top5=top5))\n    writer.add_scalar('train_loss', losses.avg, epoch)\n    writer.add_scalar('train_top1_acc', top1.avg, epoch)\n    writer.add_scalar('train_top5_acc', top5.avg, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.305983Z","iopub.execute_input":"2025-09-09T12:06:45.306178Z","iopub.status.idle":"2025-09-09T12:06:45.313395Z","shell.execute_reply.started":"2025-09-09T12:06:45.306163Z","shell.execute_reply":"2025-09-09T12:06:45.312723Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def validate(epoch, encoder, decoder, cross_entropy_loss, data_loader, word_dict, alpha_c, log_interval, writer):\n    encoder.eval()\n    decoder.eval()\n\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    references = []\n    hypotheses = []\n\n    with torch.no_grad():\n        for batch_idx, (imgs, captions, captions_len, all_captions) in enumerate(data_loader):\n            imgs, captions = imgs.to(device), captions.to(device)\n            img_features = encoder(imgs)\n            preds, alphas = decoder(img_features, captions)\n            targets = captions[:, 1:]\n\n            # Unpad sequences for loss calculation\n            targets = pack_padded_sequence(targets, captions_len, batch_first=True, enforce_sorted=False)[0]\n            packed_preds = pack_padded_sequence(preds, captions_len, batch_first=True, enforce_sorted=False)[0]\n\n            att_regularization = alpha_c * ((1 - alphas.sum(1))**2).mean()\n            loss = cross_entropy_loss(packed_preds, targets)\n            loss += att_regularization\n\n            total_caption_length = calculate_caption_lengths(word_dict, captions)\n            acc1 = accuracy(packed_preds, targets, 1)\n            acc5 = accuracy(packed_preds, targets, 5)\n            losses.update(loss.item(), total_caption_length)\n            top1.update(acc1, total_caption_length)\n            top5.update(acc5, total_caption_length)\n\n            # References (GT captions)\n            for cap_set in all_captions:\n                caps = []\n                for caption in cap_set:\n                    cap = [word_idx for word_idx in caption\n                                    if word_idx != word_dict['<start>'] and word_idx != word_dict['<pad>']]\n                    caps.append(cap)\n                references.append(caps)\n\n            # Hypotheses (Predicted captions with BEAM SEARCH)\n            for i in range(imgs.size(0)):\n                sentence, alpha = decoder.caption(img_features[i].unsqueeze(0), beam_size=3)  \n                hypothesis = [idx for idx in sentence if idx not in (word_dict['<start>'], word_dict['<pad>'])]\n                hypotheses.append(hypothesis)\n\n            if batch_idx % log_interval == 0:\n                print('Validation Batch: [{0}/{1}]\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top 1 Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Top 5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n                          batch_idx, len(data_loader), loss=losses, top1=top1, top5=top5))\n\n        # Write scalars\n        writer.add_scalar('val_loss', losses.avg, epoch)\n        writer.add_scalar('val_top1_acc', top1.avg, epoch)\n        writer.add_scalar('val_top5_acc', top5.avg, epoch)\n\n        # BLEU scores\n        bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n        bleu_2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n        bleu_3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n        bleu_4 = corpus_bleu(references, hypotheses)\n\n        writer.add_scalar('val_bleu1', bleu_1, epoch)\n        writer.add_scalar('val_bleu2', bleu_2, epoch)\n        writer.add_scalar('val_bleu3', bleu_3, epoch)\n        writer.add_scalar('val_bleu4', bleu_4, epoch)\n\n        print('Validation Epoch: {}\\t'\n              'BLEU-1 ({})\\t'\n              'BLEU-2 ({})\\t'\n              'BLEU-3 ({})\\t'\n              'BLEU-4 ({})\\t'.format(epoch, bleu_1, bleu_2, bleu_3, bleu_4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.315401Z","iopub.execute_input":"2025-09-09T12:06:45.315588Z","iopub.status.idle":"2025-09-09T12:06:45.332299Z","shell.execute_reply.started":"2025-09-09T12:06:45.315574Z","shell.execute_reply":"2025-09-09T12:06:45.331674Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print('Starting training with {}'.format(args))\nfor epoch in range(1, args.epochs + 1):\n    scheduler.step()\n    train(epoch, encoder, decoder, optimizer, cross_entropy_loss,\n          train_loader, word_dict, args.alpha_c, args.log_interval, writer)\n    validate(epoch, encoder, decoder, cross_entropy_loss, val_loader,\n             word_dict, args.alpha_c, args.log_interval, writer)\n    model_file = '/kaggle/working/' + args.network + '_' + str(epoch) + '.pth'\n    torch.save(decoder.state_dict(), model_file)\n    print('Saved model to ' + model_file)\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:06:45.332972Z","iopub.execute_input":"2025-09-09T12:06:45.333341Z","execution_failed":"2025-09-09T12:29:58.329Z"}},"outputs":[{"name":"stdout","text":"Starting training with Namespace(batch_size=64, epochs=10, lr=0.001, step_size=5, alpha_c=1, log_interval=100, data='/kaggle/input/image-captioning-dataset', network='vgg19', model=None, tf=False)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train Batch: [0/569]\tLoss 9.2370 (9.2370)\tTop 1 Accuracy 0.000 (0.000)\tTop 5 Accuracy 0.125 (0.125)\nTrain Batch: [100/569]\tLoss 5.4839 (5.8510)\tTop 1 Accuracy 20.528 (16.160)\tTop 5 Accuracy 40.216 (37.807)\nTrain Batch: [200/569]\tLoss 5.4329 (5.6300)\tTop 1 Accuracy 17.626 (17.088)\tTop 5 Accuracy 41.127 (39.616)\nTrain Batch: [300/569]\tLoss 5.3154 (5.5243)\tTop 1 Accuracy 21.366 (17.592)\tTop 5 Accuracy 43.478 (40.533)\nTrain Batch: [400/569]\tLoss 5.1999 (5.4508)\tTop 1 Accuracy 18.442 (17.882)\tTop 5 Accuracy 43.986 (41.185)\nTrain Batch: [500/569]\tLoss 5.1847 (5.3929)\tTop 1 Accuracy 19.332 (18.056)\tTop 5 Accuracy 41.766 (41.667)\nValidation Batch: [0/64]\tLoss 4.9511 (4.9511)\tTop 1 Accuracy 20.449 (20.449)\tTop 5 Accuracy 45.262 (45.262)\nValidation Epoch: 1\tBLEU-1 (0.3856281620576739)\tBLEU-2 (0.22910143031053684)\tBLEU-3 (0.0966502387022368)\tBLEU-4 (0.0443747190254995)\t\nSaved model to /kaggle/working/vgg19_1.pth\nTrain Batch: [0/569]\tLoss 5.0303 (5.0303)\tTop 1 Accuracy 20.256 (20.256)\tTop 5 Accuracy 44.359 (44.359)\nTrain Batch: [100/569]\tLoss 4.9665 (5.0394)\tTop 1 Accuracy 18.711 (19.170)\tTop 5 Accuracy 45.725 (44.271)\nTrain Batch: [200/569]\tLoss 5.0230 (5.0323)\tTop 1 Accuracy 18.655 (19.076)\tTop 5 Accuracy 44.924 (44.301)\nTrain Batch: [300/569]\tLoss 5.0822 (5.0179)\tTop 1 Accuracy 20.787 (19.074)\tTop 5 Accuracy 45.018 (44.355)\nTrain Batch: [400/569]\tLoss 4.9093 (5.0095)\tTop 1 Accuracy 20.297 (19.119)\tTop 5 Accuracy 44.307 (44.383)\nTrain Batch: [500/569]\tLoss 5.0434 (4.9978)\tTop 1 Accuracy 18.235 (19.171)\tTop 5 Accuracy 45.761 (44.459)\nValidation Batch: [0/64]\tLoss 4.7561 (4.7561)\tTop 1 Accuracy 21.420 (21.420)\tTop 5 Accuracy 46.391 (46.391)\nValidation Epoch: 2\tBLEU-1 (0.3828225727291001)\tBLEU-2 (0.22688544806297323)\tBLEU-3 (0.10211951533537494)\tBLEU-4 (0.05046498369769308)\t\nSaved model to /kaggle/working/vgg19_2.pth\nTrain Batch: [0/569]\tLoss 4.7855 (4.7855)\tTop 1 Accuracy 19.355 (19.355)\tTop 5 Accuracy 47.097 (47.097)\n","output_type":"stream"}],"execution_count":null}]}