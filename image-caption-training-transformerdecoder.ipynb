{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12971750,"sourceType":"datasetVersion","datasetId":8204070},{"sourceId":12984839,"sourceType":"datasetVersion","datasetId":8171424},{"sourceId":13007869,"sourceType":"datasetVersion","datasetId":8171456}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:39.909664Z","iopub.execute_input":"2025-09-09T11:23:39.910251Z","iopub.status.idle":"2025-09-09T11:23:43.119839Z","shell.execute_reply.started":"2025-09-09T11:23:39.910227Z","shell.execute_reply":"2025-09-09T11:23:43.119097Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tensorboardX import SummaryWriter\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import transforms\nimport argparse, json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom nltk.translate.bleu_score import corpus_bleu\n\n\nimport sys\nsys.path.append(\"/kaggle/input/train-requirements\")\nfrom dataset import ImageCaptionDataset\nimport sys\nsys.path.append(\"/kaggle/input/transformer\")\nfrom decoder import Decoder\nfrom transformerDecoder import DecoderOnlyTransformer\nfrom encoder import Encoder\nfrom utils import AverageMeter, accuracy, calculate_caption_lengths, collate_fn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:43.120797Z","iopub.execute_input":"2025-09-09T11:23:43.121010Z","iopub.status.idle":"2025-09-09T11:23:53.270547Z","shell.execute_reply.started":"2025-09-09T11:23:43.120988Z","shell.execute_reply":"2025-09-09T11:23:53.269994Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:53.271254Z","iopub.execute_input":"2025-09-09T11:23:53.271631Z","iopub.status.idle":"2025-09-09T11:23:53.306742Z","shell.execute_reply.started":"2025-09-09T11:23:53.271611Z","shell.execute_reply":"2025-09-09T11:23:53.305924Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='Show, Attend and Tell')\nparser.add_argument('--batch-size', type=int, default=32, metavar='N',\n                    help='batch size for training (default: 64)')\nparser.add_argument('--epochs', type=int, default=10, metavar='E',\n                    help='number of epochs to train for (default: 10)')\nparser.add_argument('--lr', type=float, default=1e-4, metavar='LR',\n                    help='learning rate of the decoder (default: 1e-4)')\nparser.add_argument('--step-size', type=int, default=5,\n                    help='step size for learning rate annealing (default: 5)')\nparser.add_argument('--alpha-c', type=float, default=1, metavar='A',\n                    help='regularization constant (default: 1)')\nparser.add_argument('--log-interval', type=int, default=100, metavar='L',\n                    help='number of batches to wait before logging training stats (default: 100)')\nparser.add_argument('--data', type=str, default='/kaggle/input/image-captioning-dataset',\n                    help='path to data images (default: /kaggle/input/image-captioning-dataset)')\nparser.add_argument('--network', choices=['vgg19', 'resnet152', 'densenet161'], default='vgg19',\n                    help='Network to use in the encoder (default: vgg19)')\nparser.add_argument('--model', type=str, help='path to model')\nparser.add_argument('--tf', action='store_true', default=False,\n                    help='Use teacher forcing when training LSTM (default: False)')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:53.308927Z","iopub.execute_input":"2025-09-09T11:23:53.309213Z","iopub.status.idle":"2025-09-09T11:23:53.326406Z","shell.execute_reply.started":"2025-09-09T11:23:53.309183Z","shell.execute_reply":"2025-09-09T11:23:53.325743Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"_StoreTrueAction(option_strings=['--tf'], dest='tf', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Use teacher forcing when training LSTM (default: False)', metavar=None)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"args, unknown = parser.parse_known_args()\n\nprint(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:53.327070Z","iopub.execute_input":"2025-09-09T11:23:53.327259Z","iopub.status.idle":"2025-09-09T11:23:53.341747Z","shell.execute_reply.started":"2025-09-09T11:23:53.327243Z","shell.execute_reply":"2025-09-09T11:23:53.341033Z"}},"outputs":[{"name":"stdout","text":"Namespace(batch_size=32, epochs=10, lr=0.0001, step_size=5, alpha_c=1, log_interval=100, data='/kaggle/input/image-captioning-dataset', network='vgg19', model=None, tf=False)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tf = True # teacher forcing\nnetwork = \"resnet152\" # other options [\"densenet161\", \"vgg19\"]\narg_lr = 0.0001\nstep_size = 5\ndata = \"/kaggle/input/image-captioning-dataset/data\"\nbatch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:53.342385Z","iopub.execute_input":"2025-09-09T11:23:53.342547Z","iopub.status.idle":"2025-09-09T11:23:53.356587Z","shell.execute_reply.started":"2025-09-09T11:23:53.342535Z","shell.execute_reply":"2025-09-09T11:23:53.355842Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"writer = SummaryWriter()\n\nword_dict = json.load(open(\"/kaggle/input/image-captioning-dataset/word_dict.json\", 'r'))\nvocabulary_size = len(word_dict)\n\nencoder = Encoder(args.network)\n# decoder = Decoder(vocabulary_size, encoder.dim, args.tf)\ndecoder = DecoderOnlyTransformer(vocabulary_size, d_model=512, max_len=44, num_heads=8, device = device, encoder_dim=encoder.dim)\n\nencoder.to(device)\ndecoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:53.357308Z","iopub.execute_input":"2025-09-09T11:23:53.357522Z","iopub.status.idle":"2025-09-09T11:23:55.772640Z","shell.execute_reply.started":"2025-09-09T11:23:53.357499Z","shell.execute_reply":"2025-09-09T11:23:55.772007Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DecoderOnlyTransformer(\n  (token_embedding): Embedding(5507, 512)\n  (position_encoding): PositionEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder_layers): ModuleList(\n    (0-5): 6 x TransformerDecoderLayer(\n      (multi_head_attention): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=False)\n        (W_k): Linear(in_features=512, out_features=512, bias=False)\n        (W_v): Linear(in_features=512, out_features=512, bias=False)\n        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (cross_attention): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=False)\n        (W_k): Linear(in_features=512, out_features=512, bias=False)\n        (W_v): Linear(in_features=512, out_features=512, bias=False)\n        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (ffn): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=2048, out_features=512, bias=True)\n        (3): Dropout(p=0.1, inplace=False)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (final_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (encoder_projection): Linear(in_features=512, out_features=512, bias=True)\n  (output_projection): Linear(in_features=512, out_features=5507, bias=True)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"optimizer = optim.Adam(decoder.parameters(), lr=args.lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size)\ncross_entropy_loss = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:55.773374Z","iopub.execute_input":"2025-09-09T11:23:55.773592Z","iopub.status.idle":"2025-09-09T11:23:55.778220Z","shell.execute_reply.started":"2025-09-09T11:23:55.773574Z","shell.execute_reply":"2025-09-09T11:23:55.777609Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:55.778936Z","iopub.execute_input":"2025-09-09T11:23:55.779142Z","iopub.status.idle":"2025-09-09T11:23:55.794293Z","shell.execute_reply.started":"2025-09-09T11:23:55.779126Z","shell.execute_reply":"2025-09-09T11:23:55.793581Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# print(decoder.state_dict())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:55.795040Z","iopub.execute_input":"2025-09-09T11:23:55.795312Z","iopub.status.idle":"2025-09-09T11:23:55.809812Z","shell.execute_reply.started":"2025-09-09T11:23:55.795293Z","shell.execute_reply":"2025-09-09T11:23:55.809198Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    ImageCaptionDataset(data_transforms, args.data),\n    batch_size=args.batch_size, shuffle=True, num_workers=1)\n\nval_loader = torch.utils.data.DataLoader(\n    ImageCaptionDataset(data_transforms, args.data, split_type='val'),\n    batch_size=args.batch_size, shuffle=True, num_workers=1, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:55.810572Z","iopub.execute_input":"2025-09-09T11:23:55.811259Z","iopub.status.idle":"2025-09-09T11:23:56.019447Z","shell.execute_reply.started":"2025-09-09T11:23:55.811235Z","shell.execute_reply":"2025-09-09T11:23:56.018902Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train(epoch, encoder, decoder, optimizer, cross_entropy_loss, data_loader, word_dict, alpha_c, log_interval, writer):\n    encoder.eval()\n    decoder.train()\n\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    for batch_idx, (imgs, captions, captions_len) in enumerate(data_loader):\n        imgs, captions = imgs.to(device), captions.to(device)\n        img_features = encoder(imgs)\n        optimizer.zero_grad()\n        preds, alphas = decoder(captions, img_features)\n        targets = captions[:, 1:]\n\n        caption_lengths = [ (cap != word_dict['<pad>']).sum().item() - 1 for cap in captions ]\n        targets = pack_padded_sequence(targets, caption_lengths, batch_first=True, enforce_sorted=False)[0]\n        preds = pack_padded_sequence(preds, caption_lengths, batch_first=True, enforce_sorted=False)[0]\n\n        att_regularization = alpha_c * ((1 - alphas.sum(1))**2).mean()\n\n        loss = cross_entropy_loss(preds, targets)\n        loss += att_regularization\n        loss.backward()\n        optimizer.step()\n\n        total_caption_length = calculate_caption_lengths(word_dict, captions)\n        acc1 = accuracy(preds, targets, 1)\n        acc5 = accuracy(preds, targets, 5)\n        losses.update(loss.item(), total_caption_length)\n        top1.update(acc1, total_caption_length)\n        top5.update(acc5, total_caption_length)\n\n        if batch_idx % log_interval == 0:\n            print('Train Batch: [{0}/{1}]\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top 1 Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'\n                  'Top 5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n                      batch_idx, len(data_loader), loss=losses, top1=top1, top5=top5))\n    writer.add_scalar('train_loss', losses.avg, epoch)\n    writer.add_scalar('train_top1_acc', top1.avg, epoch)\n    writer.add_scalar('train_top5_acc', top5.avg, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:24:51.690177Z","iopub.execute_input":"2025-09-09T11:24:51.690725Z","iopub.status.idle":"2025-09-09T11:24:51.698562Z","shell.execute_reply.started":"2025-09-09T11:24:51.690694Z","shell.execute_reply":"2025-09-09T11:24:51.697795Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def validate(epoch, encoder, decoder, cross_entropy_loss, data_loader, word_dict, alpha_c, log_interval, writer):\n    encoder.eval()\n    decoder.eval()\n\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # used for calculating bleu scores\n    references = []\n    hypotheses = []\n    with torch.no_grad():\n        for batch_idx, (imgs, captions, captions_len, all_captions) in enumerate(data_loader):\n            imgs, captions = imgs.to(device), captions.to(device)\n            img_features = encoder(imgs)\n            preds, alphas = decoder(captions, img_features)\n            targets = captions[:, 1:]\n            # print(f\"cap={captions.size()}\")\n            # print((targets.size()))\n            # print((preds.size()))\n            targets = pack_padded_sequence(targets, captions_len, batch_first=True, enforce_sorted=False)[0]\n            packed_preds = pack_padded_sequence(preds, captions_len, batch_first=True, enforce_sorted=False)[0]\n\n            att_regularization = alpha_c * ((1 - alphas.sum(1))**2).mean()\n            # print((targets.size()))\n            # print((packed_preds.size()))\n            loss = cross_entropy_loss(packed_preds, targets)\n            loss += att_regularization\n\n            total_caption_length = calculate_caption_lengths(word_dict, captions)\n            acc1 = accuracy(packed_preds, targets, 1)\n            acc5 = accuracy(packed_preds, targets, 5)\n            losses.update(loss.item(), total_caption_length)\n            top1.update(acc1, total_caption_length)\n            top5.update(acc5, total_caption_length)\n\n            for cap_set in all_captions:\n                caps = []\n                for caption in cap_set:\n                    cap = [word_idx for word_idx in caption\n                                    if word_idx != word_dict['<start>'] and word_idx != word_dict['<pad>']]\n                    caps.append(cap)\n                references.append(caps)\n\n            word_idxs = torch.max(preds, dim=2)[1]\n            for idxs in word_idxs.tolist():\n                hypotheses.append([idx for idx in idxs\n                                       if idx != word_dict['<start>'] and idx != word_dict['<pad>']])\n\n            if batch_idx % log_interval == 0:\n                print('Validation Batch: [{0}/{1}]\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top 1 Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Top 5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n                          batch_idx, len(data_loader), loss=losses, top1=top1, top5=top5))\n        writer.add_scalar('val_loss', losses.avg, epoch)\n        writer.add_scalar('val_top1_acc', top1.avg, epoch)\n        writer.add_scalar('val_top5_acc', top5.avg, epoch)\n\n        bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n        bleu_2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n        bleu_3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n        bleu_4 = corpus_bleu(references, hypotheses)\n\n        writer.add_scalar('val_bleu1', bleu_1, epoch)\n        writer.add_scalar('val_bleu2', bleu_2, epoch)\n        writer.add_scalar('val_bleu3', bleu_3, epoch)\n        writer.add_scalar('val_bleu4', bleu_4, epoch)\n        print('Validation Epoch: {}\\t'\n              'BLEU-1 ({})\\t'\n              'BLEU-2 ({})\\t'\n              'BLEU-3 ({})\\t'\n              'BLEU-4 ({})\\t'.format(epoch, bleu_1, bleu_2, bleu_3, bleu_4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:23:56.030283Z","iopub.execute_input":"2025-09-09T11:23:56.030495Z","iopub.status.idle":"2025-09-09T11:23:56.047738Z","shell.execute_reply.started":"2025-09-09T11:23:56.030470Z","shell.execute_reply":"2025-09-09T11:23:56.047061Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print('Starting training with {}'.format(args))\nfor epoch in range(1, args.epochs + 1):\n    scheduler.step()\n    train(epoch, encoder, decoder, optimizer, cross_entropy_loss,\n          train_loader, word_dict, args.alpha_c, args.log_interval, writer)\n    validate(epoch, encoder, decoder, cross_entropy_loss, val_loader,\n             word_dict, args.alpha_c, args.log_interval, writer)\n    model_file = '/kaggle/working/' + args.network + '_' + str(epoch) + '.pth'\n    torch.save(decoder.state_dict(), model_file)\n    print('Saved model to ' + model_file)\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T11:24:57.410409Z","iopub.execute_input":"2025-09-09T11:24:57.411147Z","execution_failed":"2025-09-09T11:39:38.662Z"}},"outputs":[{"name":"stdout","text":"Starting training with Namespace(batch_size=32, epochs=10, lr=0.0001, step_size=5, alpha_c=1, log_interval=100, data='/kaggle/input/image-captioning-dataset', network='vgg19', model=None, tf=False)\nTrain Batch: [0/1138]\tLoss 9.3726 (9.3726)\tTop 1 Accuracy 0.000 (0.000)\tTop 5 Accuracy 0.255 (0.255)\nTrain Batch: [100/1138]\tLoss 5.9067 (6.6313)\tTop 1 Accuracy 15.309 (10.909)\tTop 5 Accuracy 36.790 (32.038)\nTrain Batch: [200/1138]\tLoss 5.6650 (6.1209)\tTop 1 Accuracy 16.704 (14.353)\tTop 5 Accuracy 40.312 (36.191)\nTrain Batch: [300/1138]\tLoss 5.3680 (5.8905)\tTop 1 Accuracy 19.213 (15.879)\tTop 5 Accuracy 40.972 (38.323)\nTrain Batch: [400/1138]\tLoss 5.1035 (5.7173)\tTop 1 Accuracy 25.648 (18.032)\tTop 5 Accuracy 44.301 (39.888)\nTrain Batch: [500/1138]\tLoss 5.0057 (5.5886)\tTop 1 Accuracy 27.340 (19.580)\tTop 5 Accuracy 46.798 (41.053)\nTrain Batch: [600/1138]\tLoss 4.6455 (5.4839)\tTop 1 Accuracy 29.155 (20.694)\tTop 5 Accuracy 48.688 (42.017)\nTrain Batch: [700/1138]\tLoss 4.7755 (5.3965)\tTop 1 Accuracy 26.879 (21.665)\tTop 5 Accuracy 48.519 (42.871)\nTrain Batch: [800/1138]\tLoss 4.6980 (5.3142)\tTop 1 Accuracy 29.471 (22.644)\tTop 5 Accuracy 52.645 (43.761)\nTrain Batch: [900/1138]\tLoss 4.4217 (5.2324)\tTop 1 Accuracy 35.171 (23.603)\tTop 5 Accuracy 55.381 (44.722)\nTrain Batch: [1000/1138]\tLoss 4.4732 (5.1564)\tTop 1 Accuracy 33.628 (24.484)\tTop 5 Accuracy 55.310 (45.726)\nTrain Batch: [1100/1138]\tLoss 4.5212 (5.0886)\tTop 1 Accuracy 35.106 (25.249)\tTop 5 Accuracy 52.660 (46.594)\nValidation Batch: [0/127]\tLoss 4.3780 (4.3780)\tTop 1 Accuracy 32.477 (32.477)\tTop 5 Accuracy 55.841 (55.841)\nValidation Batch: [100/127]\tLoss 4.0330 (4.2951)\tTop 1 Accuracy 36.597 (34.070)\tTop 5 Accuracy 62.238 (57.210)\nValidation Epoch: 1\tBLEU-1 (0.12604480294791712)\tBLEU-2 (0.0725742422834723)\tBLEU-3 (0.03533256533315609)\tBLEU-4 (0.01759638660618823)\t\nSaved model to /kaggle/working/vgg19_1.pth\nTrain Batch: [0/1138]\tLoss 4.3684 (4.3684)\tTop 1 Accuracy 33.034 (33.034)\tTop 5 Accuracy 55.281 (55.281)\nTrain Batch: [100/1138]\tLoss 4.4525 (4.2606)\tTop 1 Accuracy 35.768 (34.261)\tTop 5 Accuracy 52.897 (57.343)\nTrain Batch: [200/1138]\tLoss 4.6717 (4.2276)\tTop 1 Accuracy 31.494 (34.538)\tTop 5 Accuracy 52.414 (57.665)\nTrain Batch: [300/1138]\tLoss 4.3328 (4.1996)\tTop 1 Accuracy 30.805 (34.842)\tTop 5 Accuracy 56.092 (58.031)\nTrain Batch: [400/1138]\tLoss 4.0664 (4.1797)\tTop 1 Accuracy 35.492 (34.992)\tTop 5 Accuracy 61.140 (58.246)\nTrain Batch: [500/1138]\tLoss 3.9503 (4.1595)\tTop 1 Accuracy 37.441 (35.223)\tTop 5 Accuracy 63.270 (58.563)\nTrain Batch: [600/1138]\tLoss 4.0893 (4.1402)\tTop 1 Accuracy 37.972 (35.410)\tTop 5 Accuracy 58.726 (58.823)\nTrain Batch: [700/1138]\tLoss 4.0513 (4.1190)\tTop 1 Accuracy 36.031 (35.607)\tTop 5 Accuracy 59.008 (59.113)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}